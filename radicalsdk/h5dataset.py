# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_h5dataset.ipynb (unless otherwise specified).

__all__ = ['logger', 'H5DatasetLoader']

# Cell

import numpy as np
import h5py
import tensorflow as tf

import mmwave

import logging

logger = logging.getLogger()

# Cell

class H5DatasetLoader(object):
    """A thin wrapper around h5py to provide convenience functions for training"""
    def __init__(self, filenames):
        super(H5DatasetLoader, self).__init__()
        self.filenames = filenames
        if isinstance(self.filenames, list):
            raise NotImplementedError
        else:
            self.h5_file = h5py.File(self.filenames, 'r')
        self.streams_available = list(self.h5_file.keys())

    def __len__(self):
        return len(self.h5_file[self.streams_available[0]])

    def __getitem__(self, stream):
        return self.h5_file[stream]

    @property
    def filename(self):
        return self.filenames

    def get_tf_dataset(self,
                       streams=['radar', 'rgb', 'depth'],
                       shuffle=False,
                       repeat=False,
                       batchsize=16,
                       preprocess_chain=None,
                       prefetch=2,
                       flatten_single=False,
                      ):
        logger.debug("Tensorflow Dataset creation")

        out_shapes = tuple([
            tf.TensorShape(list(self.h5_file[s].shape[1:])) for s in streams
        ])
        out_types = tuple([self.h5_file[s].dtype for s in streams])

        def _gen():
            for i in range(len(self.h5_file[streams[0]])):
                yield tuple(self.h5_file[s][i] for s in streams)

        _dataset = tf.data.Dataset.from_generator(
            _gen,
            output_types = out_types,
            output_shapes = out_shapes,
        )

        if shuffle:
            debug("  Outputs of dataset will be shuffled")
            _dataset = _dataset.shuffle(batchsize * 4)

        if repeat:
            debug(f'  Dataset will be repeated {repeat} files')
            _dataset = _dataset.repeat(repeat)

        if preprocess_chain is not None:
            for op in preprocess_chain:
                _dataset = _dataset.map(op)

        if flatten_single:
            assert(len(streams) == 1)
            logger.debug("  Flattening shapes for single stream inference")
            logger.debug(_dataset)
            _dataset = _dataset.map(lambda x: x)
            logger.debug(_dataset)

        _dataset = _dataset.batch(batchsize)


        if prefetch:
            _dataset = _dataset.prefetch(prefetch)

        return _dataset
